{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cornac\n",
    "import pandas as pd\n",
    "\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/microsoft/recommenders/blob/98d661edc6a9965c7f42b76dc5317af3ae74d5e0/recommenders/models/cornac/cornac_utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Default column names\n",
    "DEFAULT_USER_COL = \"user_id\"\n",
    "DEFAULT_ITEM_COL = \"business_id\"\n",
    "DEFAULT_RATING_COL = \"rating\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "DEFAULT_SIMILARITY_COL = \"sim\"\n",
    "COL_DICT = {\n",
    "    \"col_user\": DEFAULT_USER_COL,\n",
    "    \"col_item\": DEFAULT_ITEM_COL,\n",
    "    \"col_rating\": DEFAULT_RATING_COL,\n",
    "    \"col_prediction\": DEFAULT_PREDICTION_COL,\n",
    "}\n",
    "\n",
    "# Filtering variables\n",
    "DEFAULT_K = 10\n",
    "DEFAULT_THRESHOLD = 10\n",
    "\n",
    "# Other\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model,\n",
    "    data,\n",
    "    usercol=DEFAULT_USER_COL,\n",
    "    itemcol=DEFAULT_ITEM_COL,\n",
    "    predcol=DEFAULT_PREDICTION_COL,\n",
    "):\n",
    "    \"\"\"Computes predictions of a recommender model from Cornac on the data.\n",
    "    Can be used for computing rating metrics like RMSE.\n",
    "\n",
    "    Args:\n",
    "        model (cornac.models.Recommender): A recommender model from Cornac\n",
    "        data (pandas.DataFrame): The data on which to predict\n",
    "        usercol (str): Name of the user column\n",
    "        itemcol (str): Name of the item column\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe with usercol, itemcol, predcol\n",
    "    \"\"\"\n",
    "    uid_map = model.train_set.uid_map\n",
    "    iid_map = model.train_set.iid_map\n",
    "    predictions = [\n",
    "        [\n",
    "            getattr(row, usercol),\n",
    "            getattr(row, itemcol),\n",
    "            model.rate(\n",
    "                user_idx=uid_map.get(getattr(row, usercol), len(uid_map)),\n",
    "                item_idx=iid_map.get(getattr(row, itemcol), len(iid_map)),\n",
    "            ),\n",
    "        ]\n",
    "        for row in data.itertuples()\n",
    "    ]\n",
    "    predictions = pd.DataFrame(data=predictions, columns=[usercol, itemcol, predcol])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_ranking(\n",
    "    model,\n",
    "    data,\n",
    "    usercol=DEFAULT_USER_COL,\n",
    "    itemcol=DEFAULT_ITEM_COL,\n",
    "    predcol=DEFAULT_PREDICTION_COL,\n",
    "    remove_seen=False,\n",
    "):\n",
    "    \"\"\"Computes predictions of recommender model from Cornac on all users and items in data.\n",
    "    It can be used for computing ranking metrics like NDCG.\n",
    "\n",
    "    Args:\n",
    "        model (cornac.models.Recommender): A recommender model from Cornac\n",
    "        data (pandas.DataFrame): The data from which to get the users and items\n",
    "        usercol (str): Name of the user column\n",
    "        itemcol (str): Name of the item column\n",
    "        remove_seen (bool): Flag to remove (user, item) pairs seen in the training data\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe with usercol, itemcol, predcol\n",
    "    \"\"\"\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(model.train_set.iid_map.keys())\n",
    "    for uid, user_idx in model.train_set.uid_map.items():\n",
    "        user = [uid] * len(item)\n",
    "        users.extend(user)\n",
    "        items.extend(item)\n",
    "        preds.extend(model.score(user_idx).tolist())\n",
    "\n",
    "    all_predictions = pd.DataFrame(\n",
    "        data={usercol: users, itemcol: items, predcol: preds}\n",
    "    )\n",
    "\n",
    "    if remove_seen:\n",
    "        tempdf = pd.concat(\n",
    "            [\n",
    "                data[[usercol, itemcol]],\n",
    "                pd.DataFrame(\n",
    "                    data=np.ones(data.shape[0]), columns=[\"dummycol\"], index=data.index\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        merged = pd.merge(tempdf, all_predictions, on=[usercol, itemcol], how=\"outer\")\n",
    "        return merged[merged[\"dummycol\"].isnull()].drop(\"dummycol\", axis=1)\n",
    "    else:\n",
    "        return all_predictions\n",
    "\n",
    "\n",
    "# map_at_k, precision_at_k, recall_at_k\n",
    "\n",
    "\n",
    "def get_top_k_items(\n",
    "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
    "):\n",
    "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
    "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
    "    for each user.\n",
    "\n",
    "    Note:\n",
    "        If it is implicit rating, just append a column of constants to be\n",
    "        ratings.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
    "        customerID-itemID-rating)\n",
    "        col_user (str): column name for user\n",
    "        col_rating (str): column name for rating\n",
    "        k (int or None): number of items for each user; None means that the input has already been\n",
    "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
    "    \"\"\"\n",
    "    # Sort dataframe by col_user and (top k) col_rating\n",
    "    if k is None:\n",
    "        top_k_items = dataframe\n",
    "    else:\n",
    "        top_k_items = (\n",
    "            dataframe.sort_values([col_user, col_rating], ascending=[True, False])\n",
    "            .groupby(col_user, as_index=False)\n",
    "            .head(k)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    # Add ranks\n",
    "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "def _get_rating_column(relevancy_method: str, **kwargs) -> str:\n",
    "    r\"\"\"Helper utility to simplify the arguments of eval metrics\n",
    "    Attemtps to address https://github.com/microsoft/recommenders/issues/1737.\n",
    "\n",
    "    Args:\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "\n",
    "    Returns:\n",
    "        str: rating column name.\n",
    "    \"\"\"\n",
    "    if relevancy_method != \"top_k\":\n",
    "        if \"col_rating\" not in kwargs:\n",
    "            raise ValueError(\"Expected an argument `col_rating` but wasn't found.\")\n",
    "        col_rating = kwargs.get(\"col_rating\")\n",
    "    else:\n",
    "        col_rating = kwargs.get(\"col_rating\", DEFAULT_RATING_COL)\n",
    "    return col_rating\n",
    "\n",
    "\n",
    "def merge_ranking_true_pred(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user,\n",
    "    col_item,\n",
    "    col_rating,\n",
    "    col_prediction,\n",
    "    relevancy_method,\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "    \"\"\"Filter truth and prediction data frames on common users\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user (optional)\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame, pandas.DataFrame, int: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
    "        DataFrame of hit counts vs actual relevant items per user number of unique user ids\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the prediction and true data frames have the same set of users\n",
    "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
    "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
    "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
    "    n_users = len(common_users)\n",
    "\n",
    "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
    "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
    "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
    "    # to calculate penalized precision of the ordered items.\n",
    "    if relevancy_method == \"top_k\":\n",
    "        top_k = k\n",
    "    elif relevancy_method == \"by_threshold\":\n",
    "        top_k = threshold\n",
    "    elif relevancy_method is None:\n",
    "        top_k = None\n",
    "    else:\n",
    "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
    "    df_hit = get_top_k_items(\n",
    "        dataframe=rating_pred_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_prediction,\n",
    "        k=top_k,\n",
    "    )\n",
    "    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n",
    "        [col_user, col_item, \"rank\"]\n",
    "    ]\n",
    "\n",
    "    # count the number of hits vs actual relevant items per user\n",
    "    df_hit_count = pd.merge(\n",
    "        df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n",
    "        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
    "            {\"actual\": \"count\"}\n",
    "        ),\n",
    "        on=col_user,\n",
    "    )\n",
    "\n",
    "    return df_hit, df_hit_count, n_users\n",
    "\n",
    "\n",
    "def precision_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Precision at K.\n",
    "\n",
    "    Note:\n",
    "        We use the same formula to calculate precision@k as that in Spark.\n",
    "        More details can be found at\n",
    "        http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt\n",
    "        In particular, the maximum achievable precision may be < 1, if the number of items for a\n",
    "        user in rating_pred is less than k.\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        float: precision at k (min=0, max=1)\n",
    "    \"\"\"\n",
    "    col_rating = _get_rating_column(relevancy_method, **kwargs)\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (df_hit_count[\"hit\"] / k).sum() / n_users\n",
    "\n",
    "\n",
    "def recall_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Recall at K.\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than\n",
    "        k items exist for a user in rating_true.\n",
    "    \"\"\"\n",
    "    col_rating = _get_rating_column(relevancy_method, **kwargs)\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users\n",
    "\n",
    "\n",
    "def map_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Mean Average Precision at k\n",
    "\n",
    "    The implementation of MAP is referenced from Spark MLlib evaluation metrics.\n",
    "    https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems\n",
    "\n",
    "    A good reference can be found at:\n",
    "    http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "\n",
    "    Note:\n",
    "        1. The evaluation function is named as 'MAP is at k' because the evaluation class takes top k items for\n",
    "        the prediction items. The naming is different from Spark.\n",
    "\n",
    "        2. The MAP is meant to calculate Avg. Precision for the relevant items, so it is normalized by the number of\n",
    "        relevant items in the ground truth data, instead of k.\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        float: MAP at k (min=0, max=1).\n",
    "    \"\"\"\n",
    "    col_rating = _get_rating_column(relevancy_method, **kwargs)\n",
    "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # calculate reciprocal rank of items for each user and sum them up\n",
    "    df_hit_sorted = df_hit.copy()\n",
    "    df_hit_sorted[\"rr\"] = (\n",
    "        df_hit_sorted.groupby(col_user).cumcount() + 1\n",
    "    ) / df_hit_sorted[\"rank\"]\n",
    "    df_hit_sorted = df_hit_sorted.groupby(col_user).agg({\"rr\": \"sum\"}).reset_index()\n",
    "\n",
    "    df_merge = pd.merge(df_hit_sorted, df_hit_count, on=col_user)\n",
    "    return (df_merge[\"rr\"] / df_merge[\"actual\"]).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type_file_path = \"node_type_ID.txt\"\n",
    "node_type_index = {}  # ID: type\n",
    "users = []\n",
    "pois = []\n",
    "with open(node_type_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line_content = line.strip().split(\"\\t\")\n",
    "        node = line_content[0]\n",
    "        node_type = line_content[1]\n",
    "        node_type_index[node] = node_type\n",
    "\n",
    "        if node_type == \"P\":\n",
    "            pois.append(node)\n",
    "        elif node_type == \"U\":\n",
    "            users.append(node)\n",
    "user_map = {node_id: i for i, node_id in enumerate(users)}\n",
    "poi_map = {node_id: i for i, node_id in enumerate(pois)}\n",
    "my_pred = np.load(\"a-pred.npy\")\n",
    "\n",
    "train = pd.read_json(\"review_train_fixed.json\", orient=\"split\")[\n",
    "    [\"user_id\", \"business_id\"]\n",
    "]\n",
    "train.drop_duplicates(inplace=True)\n",
    "train[\"r\"] = 1\n",
    "\n",
    "test = pd.read_json(\"review_test_fixed.json\", orient=\"split\")[\n",
    "    [\"user_id\", \"business_id\"]\n",
    "]\n",
    "test.drop_duplicates(inplace=True)\n",
    "\n",
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0012510133449056663, 0.002951577801958651, 0.007887955211758649)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MF\n",
    "\n",
    "model = cornac.models.MF()\n",
    "model.fit(train_set)\n",
    "pred = predict_ranking(model, train)\n",
    "\n",
    "pred[\"u\"] = pred.apply(lambda row: user_map[row[\"user_id\"]], axis=1)\n",
    "pred[\"p\"] = pred.apply(lambda row: poi_map[row[\"business_id\"]], axis=1)\n",
    "npy = pred.pivot(index=\"u\", columns=\"p\", values=\"prediction\").to_numpy()\n",
    "np.save(\"mf.npy\", npy)\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k)\n",
    "eval_precision = precision_at_k(test, pred, k=k)\n",
    "eval_recall = recall_at_k(test, pred, k=k)\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0013715142401897742, 0.003060391730141458, 0.008974847704143793)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVD\n",
    "\n",
    "model = cornac.models.SVD()\n",
    "model.fit(train_set)\n",
    "pred = predict_ranking(model, train)\n",
    "\n",
    "pred[\"u\"] = pred.apply(lambda row: user_map[row[\"user_id\"]], axis=1)\n",
    "pred[\"p\"] = pred.apply(lambda row: poi_map[row[\"business_id\"]], axis=1)\n",
    "npy = pred.pivot(index=\"u\", columns=\"p\", values=\"prediction\").to_numpy()\n",
    "np.save(\"svd.npy\", npy)\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k)\n",
    "eval_precision = precision_at_k(test, pred, k=k)\n",
    "eval_recall = recall_at_k(test, pred, k=k)\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003307423827036806, 0.006950489662676824, 0.019476880645896505)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GlobalAvg\n",
    "\n",
    "model = cornac.models.GlobalAvg()\n",
    "model.fit(train_set)\n",
    "pred = predict_ranking(model, train)\n",
    "\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k)\n",
    "eval_precision = precision_at_k(test, pred, k=k)\n",
    "eval_recall = recall_at_k(test, pred, k=k)\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.007527724683373534, 0.009834058759521218, 0.03244124942006573)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NMF\n",
    "\n",
    "model = cornac.models.NMF()\n",
    "model.fit(train_set)\n",
    "pred = predict_ranking(model, train)\n",
    "\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k)\n",
    "eval_precision = precision_at_k(test, pred, k=k)\n",
    "eval_recall = recall_at_k(test, pred, k=k)\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008857796870751554, 0.010214907508161043, 0.03639250229293792)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ours\n",
    "\n",
    "pred[\"my_pred\"] = pred.apply(\n",
    "    lambda row: my_pred[user_map[row[\"user_id\"]], poi_map[row[\"business_id\"]]], axis=1\n",
    ")\n",
    "\n",
    "pred[\"u\"] = pred.apply(lambda row: user_map[row[\"user_id\"]], axis=1)\n",
    "pred[\"p\"] = pred.apply(lambda row: poi_map[row[\"business_id\"]], axis=1)\n",
    "npy = pred.pivot(index=\"u\", columns=\"p\", values=\"prediction\").to_numpy()\n",
    "np.save(\"ours.npy\", npy)\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k, col_prediction=\"my_pred\")\n",
    "eval_precision = precision_at_k(test, pred, k=k, col_prediction=\"my_pred\")\n",
    "eval_recall = recall_at_k(test, pred, k=k, col_prediction=\"my_pred\")\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.018461178029186346, 0.014200217627856367, 0.05016586989526903)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NMF+Ours\n",
    "\n",
    "pred[\"mix_pred\"] = pred[\"prediction\"] + 0.01 * pred[\"my_pred\"]\n",
    "eval_map = map_at_k(test, pred, k=k, col_prediction=\"mix_pred\")\n",
    "\n",
    "eval_precision = precision_at_k(test, pred, k=k, col_prediction=\"mix_pred\")\n",
    "eval_recall = recall_at_k(test, pred, k=k, col_prediction=\"mix_pred\")\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.023247813681894207, 0.024156692056583242, 0.08323540636308963)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BPR\n",
    "\n",
    "model = cornac.models.BPR()\n",
    "model.fit(train_set)\n",
    "pred = predict_ranking(model, train)\n",
    "\n",
    "pred[\"u\"] = pred.apply(lambda row: user_map[row[\"user_id\"]], axis=1)\n",
    "pred[\"p\"] = pred.apply(lambda row: poi_map[row[\"business_id\"]], axis=1)\n",
    "npy = pred.pivot(index=\"u\", columns=\"p\", values=\"prediction\").to_numpy()\n",
    "np.save(\"bpr.npy\", npy)\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k)\n",
    "eval_precision = precision_at_k(test, pred, k=k)\n",
    "eval_recall = recall_at_k(test, pred, k=k)\n",
    "eval_map, eval_precision, eval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.026438096775013277, 0.025979325353645267, 0.09256534167237288)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BPR+Ours\n",
    "\n",
    "pred[\"my_pred\"] = pred.apply(\n",
    "    lambda row: my_pred[user_map[row[\"user_id\"]], poi_map[row[\"business_id\"]]], axis=1\n",
    ")\n",
    "pred[\"mix_pred\"] = pred[\"prediction\"] + 0.5 * pred[\"my_pred\"]\n",
    "\n",
    "pred[\"u\"] = pred.apply(lambda row: user_map[row[\"user_id\"]], axis=1)\n",
    "pred[\"p\"] = pred.apply(lambda row: poi_map[row[\"business_id\"]], axis=1)\n",
    "npy = pred.pivot(index=\"u\", columns=\"p\", values=\"prediction\").to_numpy()\n",
    "np.save(\"bpr+ours.npy\", npy)\n",
    "\n",
    "eval_map = map_at_k(test, pred, k=k, col_prediction=\"mix_pred\")\n",
    "eval_precision = precision_at_k(test, pred, k=k, col_prediction=\"mix_pred\")\n",
    "eval_recall = recall_at_k(test, pred, k=k, col_prediction=\"mix_pred\")\n",
    "eval_map, eval_precision, eval_recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "8007",
   "language": "python",
   "name": "8007"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
